{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Import and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>great president my ass that nigger faggot used...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>trying to help people when i'm literally a dum...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>@user ching chong sheng shiong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>And now yu white nigger</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Glad that the despicable Trump fan MAGABOMBER ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  Label\n",
       "0    `- This is not ``creative``.  Those are the di...      0\n",
       "1    `  :: the term ``standard model`` is itself le...      0\n",
       "2      True or false, the situation as of March 200...      0\n",
       "3     Next, maybe you could work on being less cond...      0\n",
       "4                 This page will need disambiguation.       0\n",
       "..                                                 ...    ...\n",
       "415  great president my ass that nigger faggot used...      1\n",
       "416  trying to help people when i'm literally a dum...      1\n",
       "417                     @user ching chong sheng shiong      1\n",
       "418                            And now yu white nigger      1\n",
       "419  Glad that the despicable Trump fan MAGABOMBER ...      1\n",
       "\n",
       "[420 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Short-Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420 entries, 0 to 419\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   420 non-null    object\n",
      " 1   Label   420 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    215\n",
       "1    205\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    215\n",
       "1    205\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Converting Tweets to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    `- this is not ``creative``.  those are the di...      0\n",
      "1    `  :: the term ``standard model`` is itself le...      0\n",
      "2      true or false, the situation as of march 200...      0\n",
      "3     next, maybe you could work on being less cond...      0\n",
      "4                 this page will need disambiguation.       0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when i'm literally a dum...      1\n",
      "417                     @user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['tweet'] = df['tweet'].str.lower()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Cleaning HTML Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    `- this is not ``creative``.  those are the di...      0\n",
      "1    `  :: the term ``standard model`` is itself le...      0\n",
      "2      true or false, the situation as of march 200...      0\n",
      "3     next, maybe you could work on being less cond...      0\n",
      "4                 this page will need disambiguation.       0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when i'm literally a dum...      1\n",
      "417                     @user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salman\\AppData\\Local\\Temp\\ipykernel_11124\\3149476355.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  cleaned_tweet = BeautifulSoup(row['tweet']).text\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "for index, row in df.iterrows():\n",
    "    cleaned_tweet = BeautifulSoup(row['tweet']).text\n",
    "    df.at[index, 'tweet'] = cleaned_tweet\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    `- this is not ``creative``.  those are the di...      0\n",
      "1    `  :: the term ``standard model`` is itself le...      0\n",
      "2      true or false, the situation as of march 200...      0\n",
      "3     next, maybe you could work on being less cond...      0\n",
      "4                 this page will need disambiguation.       0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when i'm literally a dum...      1\n",
      "417                     @user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_url(tweet):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', tweet)\n",
    "df['tweet'] = df['tweet'].apply(remove_url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0     this is not creative  those are the dictionar...      0\n",
      "1       the term standard model is itself less npov...      0\n",
      "2      true or false the situation as of march 2002...      0\n",
      "3     next maybe you could work on being less conde...      0\n",
      "4                  this page will need disambiguation       0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when im literally a dumb...      1\n",
      "417                      user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "def remove_punctuation(tweet):\n",
    "    text = ''\n",
    "    for char in tweet:\n",
    "        if char not in exclude:\n",
    "            text += char\n",
    "    return text\n",
    "df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0     this is not creative  those are the dictionar...      0\n",
      "1       the term standard model is itself less npov...      0\n",
      "2      true or false the situation as of march 2002...      0\n",
      "3     next maybe you could work on being less conde...      0\n",
      "4                  this page will need disambiguation       0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when im literally a dumb...      1\n",
      "417                      user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_special_characters(tweet):\n",
    "    special_chars_pattern = re.compile(r'[^\\x00-\\x7F]')\n",
    "    cleaned_tweet = special_chars_pattern.sub('', tweet)\n",
    "    return cleaned_tweet\n",
    "df['tweet'] = df['tweet'].apply(remove_special_characters)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6) Expanding Chat Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    this is not creative those are the dictionary ...      0\n",
      "1    the term standard model is itself less npov th...      0\n",
      "2    true or false the situation as of march 2002 w...      0\n",
      "3    next maybe you could work on being less condes...      0\n",
      "4                   this page will need disambiguation      0\n",
      "..                                                 ...    ...\n",
      "415  great president my ass that nigger faggot used...      1\n",
      "416  trying to help people when im literally a dumb...      1\n",
      "417                      user ching chong sheng shiong      1\n",
      "418                            and now yu white nigger      1\n",
      "419  glad that the despicable trump fan magabomber ...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "chat_word = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon as Possible\",\n",
    "    \"ATK\": \"At the Keyboard\",\n",
    "    \"ATM\": \"At the Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back at Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By the Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get a Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"Gli\": \"Good Night\",\n",
    "    \"GNTA\": \"Great Hinds Think Alike\",\n",
    "    \"GB\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek You (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"10hi\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"18R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"IRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFH\": \"Ta-Ta For Now!\",\n",
    "    \"TTVL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U * 2\": \"1 Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\"\n",
    "}\n",
    "\n",
    "def chat_conversation(tweet):\n",
    "    new_text = []\n",
    "    for i in tweet.split():\n",
    "        if i.upper() in chat_word:\n",
    "            new_text.append(chat_word[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)\n",
    "df['tweet'] = df['tweet'].apply(chat_conversation)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7) Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def correct_spelling(tweet):\n",
    "    correct_tweet = TextBlob(tweet)\n",
    "    return correct_tweet.correct().string\n",
    "df['tweet'] = df['tweet'].apply(correct_spelling)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8) Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\salman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    creative dictionary definitions terms insuranc...      0\n",
      "1    term standard model less npov think wed prefer...      0\n",
      "2    true false situation march 2002 saudi proposal...      0\n",
      "3    next maybe could work less condescending sugge...      0\n",
      "4                             page need disambiguation      0\n",
      "..                                                 ...    ...\n",
      "415  great president ass nigger faggot used white h...      1\n",
      "416  trying help people im literally dumbass cunt c...      1\n",
      "417                      user ching chong sheng shiong      1\n",
      "418                                    yu white nigger      1\n",
      "419  glad despicable trump fan magabomber quickly f...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def remove_stop_word(tweet):\n",
    "    new_word = []\n",
    "    for word in tweet.split():\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            new_word.append(word)\n",
    "    return \" \".join(new_word)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_stop_word)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9) Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label\n",
      "0    creative dictionary definitions terms insuranc...      0\n",
      "1    term standard model less npov think wed prefer...      0\n",
      "2    true false situation march 2002 saudi proposal...      0\n",
      "3    next maybe could work less condescending sugge...      0\n",
      "4                             page need disambiguation      0\n",
      "..                                                 ...    ...\n",
      "415  great president ass nigger faggot used white h...      1\n",
      "416  trying help people im literally dumbass cunt c...      1\n",
      "417                      user ching chong sheng shiong      1\n",
      "418                                    yu white nigger      1\n",
      "419  glad despicable trump fan magabomber quickly f...      1\n",
      "\n",
      "[420 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(remove_emojis)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label  \\\n",
      "0    `- This is not ``creative``.  Those are the di...      0   \n",
      "1    `  :: the term ``standard model`` is itself le...      0   \n",
      "2      True or false, the situation as of March 200...      0   \n",
      "3     Next, maybe you could work on being less cond...      0   \n",
      "4                 This page will need disambiguation.       0   \n",
      "..                                                 ...    ...   \n",
      "415  great president my ass that nigger faggot used...      1   \n",
      "416  trying to help people when i'm literally a dum...      1   \n",
      "417                     @user ching chong sheng shiong      1   \n",
      "418                            And now yu white nigger      1   \n",
      "419  Glad that the despicable Trump fan MAGABOMBER ...      1   \n",
      "\n",
      "                                                tokens  \n",
      "0    [`, -, This, is, not, ``, creative, ``, ., Tho...  \n",
      "1    [`, :, :, the, term, ``, standard, model, ``, ...  \n",
      "2    [True, or, false, ,, the, situation, as, of, M...  \n",
      "3    [Next, ,, maybe, you, could, work, on, being, ...  \n",
      "4          [This, page, will, need, disambiguation, .]  \n",
      "..                                                 ...  \n",
      "415  [great, president, my, ass, that, nigger, fagg...  \n",
      "416  [trying, to, help, people, when, i, 'm, litera...  \n",
      "417             [@, user, ching, chong, sheng, shiong]  \n",
      "418                      [And, now, yu, white, nigger]  \n",
      "419  [Glad, that, the, despicable, Trump, fan, MAGA...  \n",
      "\n",
      "[420 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens\n",
    "df['tokens'] = df['tweet'].apply(tokenize_tweet)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11) Stemming using PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label  \\\n",
      "0    creative dictionary definitions terms insuranc...      0   \n",
      "1    term standard model less npov think wed prefer...      0   \n",
      "2    true false situation march 2002 saudi proposal...      0   \n",
      "3    next maybe could work less condescending sugge...      0   \n",
      "4                             page need disambiguation      0   \n",
      "..                                                 ...    ...   \n",
      "415  great president ass nigger faggot used white h...      1   \n",
      "416  trying help people im literally dumbass cunt c...      1   \n",
      "417                      user ching chong sheng shiong      1   \n",
      "418                                    yu white nigger      1   \n",
      "419  glad despicable trump fan magabomber quickly f...      1   \n",
      "\n",
      "                                                tokens  \n",
      "0    [creative, dictionary, definitions, terms, ins...  \n",
      "1    [term, standard, model, less, npov, think, wed...  \n",
      "2    [true, false, situation, march, 2002, saudi, p...  \n",
      "3    [next, maybe, could, work, less, condescending...  \n",
      "4                         [page, need, disambiguation]  \n",
      "..                                                 ...  \n",
      "415  [great, president, ass, nigger, faggot, used, ...  \n",
      "416  [trying, help, people, im, literally, dumbass,...  \n",
      "417                [user, ching, chong, sheng, shiong]  \n",
      "418                                [yu, white, nigger]  \n",
      "419  [glad, despicable, trump, fan, magabomber, qui...  \n",
      "\n",
      "[420 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_word(tweet):\n",
    "    return \" \".join([ps.stem(word) for word in tweet.split()])\n",
    "df['tokens'] = df['tweet'].apply(tokenize_tweet)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12) Lemmatization using WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label  \\\n",
      "0    creative dictionary definitions terms insuranc...      0   \n",
      "1    term standard model less npov think wed prefer...      0   \n",
      "2    true false situation march 2002 saudi proposal...      0   \n",
      "3    next maybe could work less condescending sugge...      0   \n",
      "4                             page need disambiguation      0   \n",
      "..                                                 ...    ...   \n",
      "415  great president ass nigger faggot used white h...      1   \n",
      "416  trying help people im literally dumbass cunt c...      1   \n",
      "417                      user ching chong sheng shiong      1   \n",
      "418                                    yu white nigger      1   \n",
      "419  glad despicable trump fan magabomber quickly f...      1   \n",
      "\n",
      "                                                tokens  \n",
      "0    [creative, dictionary, definitions, terms, ins...  \n",
      "1    [term, standard, model, less, npov, think, wed...  \n",
      "2    [true, false, situation, march, 2002, saudi, p...  \n",
      "3    [next, maybe, could, work, less, condescending...  \n",
      "4                         [page, need, disambiguation]  \n",
      "..                                                 ...  \n",
      "415  [great, president, ass, nigger, faggot, used, ...  \n",
      "416  [trying, help, people, im, literally, dumbass,...  \n",
      "417                [user, ching, chong, sheng, shiong]  \n",
      "418                                [yu, white, nigger]  \n",
      "419  [glad, despicable, trump, fan, magabomber, qui...  \n",
      "\n",
      "[420 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "WordNetLemmatizer = WordNetLemmatizer()\n",
    "def stem_word(tweet):\n",
    "    return \" \".join([WordNetLemmatizer.lemmatize(word) for word in tweet.split()])\n",
    "df['tokens'] = df['tweet'].apply(tokenize_tweet)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13) POS Tagging using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tweet  Label  \\\n",
      "0    creative dictionary definitions terms insuranc...      0   \n",
      "1    term standard model less npov think wed prefer...      0   \n",
      "2    true false situation march 2002 saudi proposal...      0   \n",
      "3    next maybe could work less condescending sugge...      0   \n",
      "4                             page need disambiguation      0   \n",
      "..                                                 ...    ...   \n",
      "415  great president ass nigger faggot used white h...      1   \n",
      "416  trying help people im literally dumbass cunt c...      1   \n",
      "417                      user ching chong sheng shiong      1   \n",
      "418                                    yu white nigger      1   \n",
      "419  glad despicable trump fan magabomber quickly f...      1   \n",
      "\n",
      "                                                tokens  \\\n",
      "0    [creative, dictionary, definitions, terms, ins...   \n",
      "1    [term, standard, model, less, npov, think, wed...   \n",
      "2    [true, false, situation, march, 2002, saudi, p...   \n",
      "3    [next, maybe, could, work, less, condescending...   \n",
      "4                         [page, need, disambiguation]   \n",
      "..                                                 ...   \n",
      "415  [great, president, ass, nigger, faggot, used, ...   \n",
      "416  [trying, help, people, im, literally, dumbass,...   \n",
      "417                [user, ching, chong, sheng, shiong]   \n",
      "418                                [yu, white, nigger]   \n",
      "419  [glad, despicable, trump, fan, magabomber, qui...   \n",
      "\n",
      "                                              pos_tags  \n",
      "0    [(creative, ADJ), (dictionary, ADJ), (definiti...  \n",
      "1    [(term, NOUN), (standard, ADJ), (model, NOUN),...  \n",
      "2    [(true, ADJ), (false, ADJ), (situation, NOUN),...  \n",
      "3    [(next, ADV), (maybe, ADV), (could, AUX), (wor...  \n",
      "4    [(page, NOUN), (need, VERB), (disambiguation, ...  \n",
      "..                                                 ...  \n",
      "415  [(great, ADJ), (president, PROPN), (ass, PROPN...  \n",
      "416  [(trying, VERB), (help, NOUN), (people, NOUN),...  \n",
      "417  [(user, PROPN), (ching, PROPN), (chong, PROPN)...  \n",
      "418     [(yu, PROPN), (white, PROPN), (nigger, PROPN)]  \n",
      "419  [(glad, ADJ), (despicable, ADJ), (trump, NOUN)...  \n",
      "\n",
      "[420 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def apply_pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n",
    "df['pos_tags'] = df['tweet'].apply(apply_pos_tagging)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) CountVectorizer for Feature Extraction Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 59)\t2\n",
      "  (0, 274)\t1\n",
      "  (0, 137)\t2\n",
      "  (0, 57)\t1\n",
      "  (0, 234)\t1\n",
      "  (0, 233)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 235)\t1\n",
      "  (0, 97)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 252)\t2\n",
      "  (0, 94)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 39)\t2\n",
      "  (0, 92)\t4\n",
      "  (0, 122)\t4\n",
      "  (0, 52)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 136)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 263)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 58)\t1\n",
      "  :\t:\n",
      "  (412, 247)\t1\n",
      "  (413, 97)\t1\n",
      "  (413, 77)\t1\n",
      "  (413, 196)\t1\n",
      "  (414, 255)\t1\n",
      "  (415, 253)\t1\n",
      "  (415, 84)\t1\n",
      "  (415, 154)\t1\n",
      "  (415, 267)\t1\n",
      "  (415, 69)\t1\n",
      "  (415, 18)\t1\n",
      "  (416, 97)\t1\n",
      "  (416, 167)\t1\n",
      "  (416, 248)\t1\n",
      "  (416, 208)\t1\n",
      "  (416, 89)\t2\n",
      "  (416, 50)\t1\n",
      "  (417, 255)\t1\n",
      "  (417, 35)\t1\n",
      "  (417, 36)\t1\n",
      "  (418, 154)\t1\n",
      "  (418, 267)\t1\n",
      "  (419, 232)\t1\n",
      "  (419, 250)\t1\n",
      "  (419, 247)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english'\n",
    ")\n",
    "cv_features = cv.fit_transform(df['tweet'])\n",
    "print(cv_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) CountVectorizer for Feature Extraction (Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (9, 2)\t1\n",
      "  (27, 6)\t1\n",
      "  (28, 6)\t1\n",
      "  (32, 2)\t1\n",
      "  (33, 2)\t1\n",
      "  (40, 8)\t1\n",
      "  (47, 0)\t1\n",
      "  (52, 0)\t1\n",
      "  (53, 0)\t1\n",
      "  (58, 2)\t1\n",
      "  (61, 0)\t1\n",
      "  (64, 0)\t1\n",
      "  (65, 0)\t1\n",
      "  (68, 0)\t1\n",
      "  (69, 0)\t1\n",
      "  (72, 0)\t1\n",
      "  (73, 0)\t1\n",
      "  (74, 0)\t1\n",
      "  (75, 0)\t1\n",
      "  (76, 0)\t1\n",
      "  (80, 5)\t1\n",
      "  (83, 0)\t1\n",
      "  (86, 0)\t1\n",
      "  :\t:\n",
      "  (341, 5)\t1\n",
      "  (342, 9)\t1\n",
      "  (348, 9)\t1\n",
      "  (351, 9)\t1\n",
      "  (351, 1)\t1\n",
      "  (353, 9)\t1\n",
      "  (355, 9)\t2\n",
      "  (359, 7)\t1\n",
      "  (367, 9)\t2\n",
      "  (369, 9)\t1\n",
      "  (372, 9)\t4\n",
      "  (376, 3)\t1\n",
      "  (378, 5)\t1\n",
      "  (378, 1)\t1\n",
      "  (383, 9)\t2\n",
      "  (384, 9)\t1\n",
      "  (392, 9)\t1\n",
      "  (394, 9)\t1\n",
      "  (398, 2)\t1\n",
      "  (400, 9)\t1\n",
      "  (400, 3)\t1\n",
      "  (402, 9)\t1\n",
      "  (403, 9)\t3\n",
      "  (412, 7)\t1\n",
      "  (417, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(2, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "cv_features = cv.fit_transform(df['tweet'])\n",
    "print(cv_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 61)\t0.1728861537594601\n",
      "  (0, 284)\t0.13832422875561723\n",
      "  (0, 143)\t0.2685268470973922\n",
      "  (0, 59)\t0.13832422875561723\n",
      "  (0, 243)\t0.13074579407690437\n",
      "  (0, 242)\t0.11088911577129845\n",
      "  (0, 74)\t0.1342634235486961\n",
      "  (0, 244)\t0.13074579407690437\n",
      "  (0, 101)\t0.09506287599508885\n",
      "  (0, 15)\t0.08524545747528259\n",
      "  (0, 261)\t0.20145914878603163\n",
      "  (0, 97)\t0.10938338654064751\n",
      "  (0, 236)\t0.1160037851018548\n",
      "  (0, 66)\t0.1342634235486961\n",
      "  (0, 41)\t0.2685268470973922\n",
      "  (0, 95)\t0.47182407227936257\n",
      "  (0, 127)\t0.3320085507198559\n",
      "  (0, 54)\t0.1342634235486961\n",
      "  (0, 1)\t0.13074579407690437\n",
      "  (0, 142)\t0.12764302498748878\n",
      "  (0, 118)\t0.12764302498748878\n",
      "  (0, 44)\t0.13832422875561723\n",
      "  (0, 273)\t0.1342634235486961\n",
      "  (0, 116)\t0.09262947732445717\n",
      "  (0, 60)\t0.12764302498748878\n",
      "  :\t:\n",
      "  (413, 101)\t0.5333593934929618\n",
      "  (413, 80)\t0.5981336628935124\n",
      "  (413, 203)\t0.5981336628935124\n",
      "  (414, 264)\t1.0\n",
      "  (415, 262)\t0.36695827205604087\n",
      "  (415, 87)\t0.3973220842133241\n",
      "  (415, 161)\t0.40490734499049763\n",
      "  (415, 277)\t0.45774754009521895\n",
      "  (415, 72)\t0.3527906022165596\n",
      "  (415, 19)\t0.45774754009521895\n",
      "  (416, 101)\t0.26212057321701604\n",
      "  (416, 174)\t0.25330530547593183\n",
      "  (416, 257)\t0.3605104740458582\n",
      "  (416, 215)\t0.3814067873677332\n",
      "  (416, 92)\t0.7039101757999193\n",
      "  (416, 52)\t0.3101624612562097\n",
      "  (417, 264)\t0.2890620634502752\n",
      "  (417, 36)\t0.5461753062866616\n",
      "  (417, 38)\t0.5559386918881165\n",
      "  (417, 37)\t0.5559386918881165\n",
      "  (418, 161)\t0.6625526116276962\n",
      "  (418, 277)\t0.7490153782301932\n",
      "  (419, 241)\t0.5801668538266069\n",
      "  (419, 259)\t0.3537472569798914\n",
      "  (419, 256)\t0.7336683855122417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "tfidf_features = tfidf.fit_transform(df['tweet'])\n",
    "print(tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Word2Vec Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=4997, vector_size=100, alpha=0.025>\n",
      "(420, 100)\n",
      "[[ 6.67080879e-02  9.46860686e-02  4.83923033e-02  6.00708090e-02\n",
      "  -1.96887106e-02 -1.98439166e-01  9.63523239e-02  3.56047899e-01\n",
      "  -2.05058604e-01 -2.07049146e-01 -2.73439325e-02 -2.19710350e-01\n",
      "  -6.55084103e-03  1.15067707e-02  9.92184654e-02 -5.09678014e-02\n",
      "   3.56151909e-01 -1.64739802e-01 -1.79918826e-01 -4.33011204e-01\n",
      "   5.48086613e-02 -4.54768278e-02  3.50753605e-01 -2.47099087e-01\n",
      "   3.51511650e-02  1.61021983e-03 -2.77756572e-01  2.55365580e-01\n",
      "  -1.85798720e-01  5.89251369e-02  7.94249400e-02  7.34340958e-03\n",
      "   2.07520843e-01 -3.21074724e-01 -6.49249777e-02 -5.70996068e-02\n",
      "   1.50700763e-01 -9.39313415e-03 -6.52604774e-02  1.72344092e-02\n",
      "   1.27969354e-01 -1.30447254e-01 -1.26004964e-01  5.65673672e-02\n",
      "   2.44587019e-01 -1.52682766e-01 -2.19352543e-01 -6.22715130e-02\n",
      "   1.20453596e-01  2.43405089e-01 -3.52280885e-02 -1.33260041e-02\n",
      "   8.10433403e-02 -2.23423824e-01 -9.35083255e-02 -2.84087192e-03\n",
      "   2.03825489e-01 -5.55799529e-02  8.10376089e-03  2.56041795e-01\n",
      "  -4.60814610e-02 -3.32516097e-02  7.01522976e-02  1.18704304e-01\n",
      "  -1.35990590e-01  3.15717995e-01 -1.08214743e-01  1.53347403e-01\n",
      "  -1.88941121e-01  5.37251718e-02  1.18179791e-01  1.60110697e-01\n",
      "   2.63620883e-01  1.56628247e-02  1.06010661e-01 -4.76976335e-02\n",
      "   1.94963112e-01 -1.21702161e-02  1.18519604e-01 -1.35559604e-01\n",
      "  -1.32214412e-01  2.22300570e-02 -3.60431448e-02  3.73093545e-01\n",
      "  -5.21352142e-02 -8.88633281e-02  2.46050254e-01  1.91641256e-01\n",
      "   3.40241164e-01  1.06962405e-01  9.72801968e-02  6.40148446e-02\n",
      "  -7.15362802e-02  7.40616769e-02  3.27804476e-01  8.22528973e-02\n",
      "   1.12565272e-01 -1.96693361e-01 -2.34887190e-02 -8.03858694e-03]\n",
      " [ 5.39122857e-02  1.50988027e-01 -1.94588304e-02  4.78196368e-02\n",
      "  -2.47434713e-02 -2.55011618e-01  7.41012916e-02  3.62295419e-01\n",
      "  -2.37298250e-01 -1.75541177e-01  3.00051235e-02 -2.54659712e-01\n",
      "  -7.64888823e-02  4.01780987e-03  1.19834691e-01  1.32610826e-02\n",
      "   3.97593766e-01 -1.39494404e-01 -1.63191140e-01 -4.72999632e-01\n",
      "   7.36125708e-02 -1.04955351e-02  3.54320616e-01 -1.78813159e-01\n",
      "  -4.58872877e-03  3.02671865e-02 -3.36512893e-01  2.36668676e-01\n",
      "  -1.95169047e-01 -2.28095590e-03  1.36588171e-01  3.67237367e-02\n",
      "   2.41402522e-01 -3.48351240e-01 -1.73919760e-02 -9.31598023e-02\n",
      "   1.43139675e-01  3.76414284e-02 -8.46376717e-02  4.02027182e-02\n",
      "   7.07293302e-02 -9.61607248e-02 -1.36610731e-01  2.38946788e-02\n",
      "   2.25574985e-01 -1.42084926e-01 -2.77105421e-01 -3.87790240e-02\n",
      "   1.14763893e-01  1.86337441e-01 -1.94473304e-02 -3.74034350e-03\n",
      "   6.58047432e-03 -2.09827468e-01 -6.85904101e-02 -5.62377907e-02\n",
      "   2.39681393e-01 -1.30376592e-01 -3.57429730e-03  2.96664238e-01\n",
      "  -7.93211460e-02 -3.34066004e-02  1.50941879e-01  1.54685572e-01\n",
      "  -1.28159925e-01  3.17748040e-01 -1.20628877e-02  7.64235705e-02\n",
      "  -2.22420380e-01  5.00226542e-02  1.08432129e-01  7.95605183e-02\n",
      "   2.01978505e-01 -1.60390921e-02  1.26845732e-01  1.18178977e-02\n",
      "   2.06312418e-01  1.79876406e-02  8.44204277e-02 -1.55317217e-01\n",
      "  -1.10722281e-01  4.85047624e-02 -3.57640535e-03  3.80018741e-01\n",
      "  -8.70302841e-02 -5.96765429e-02  2.43820712e-01  2.19589636e-01\n",
      "   3.20582539e-01  1.35737821e-01  1.77605301e-01  1.25213876e-01\n",
      "  -4.21450548e-02  5.74297607e-02  3.16837817e-01  4.73574214e-02\n",
      "   1.59842208e-01 -1.48993313e-01 -5.81977628e-02  6.35687858e-02]\n",
      " [ 1.09388828e-02  2.12457940e-01  1.10689543e-01  5.03555425e-02\n",
      "   1.12680681e-01 -8.17838088e-02 -2.51222365e-02  3.32332939e-01\n",
      "  -3.09127241e-01 -3.60038131e-02 -1.51818275e-01 -2.54613876e-01\n",
      "  -1.05080018e-02 -3.25534903e-02  1.08154818e-01 -1.86306521e-01\n",
      "   2.09159523e-01 -1.79784268e-01 -6.40786588e-02 -3.92691702e-01\n",
      "   8.85132421e-03 -5.94686046e-02  3.47354561e-01 -1.01559691e-01\n",
      "   4.41576308e-03 -8.46289992e-02 -2.37530485e-01  1.54681236e-01\n",
      "  -2.22974792e-01  1.49811972e-02  2.12581486e-01 -9.08896625e-02\n",
      "   1.19747557e-01 -2.59511292e-01 -2.06191652e-02 -9.93916616e-02\n",
      "   4.51249443e-02  4.59567904e-02 -1.45458102e-01 -7.15020522e-02\n",
      "  -4.20815386e-02 -3.75836343e-02 -1.65282175e-01  2.05032721e-01\n",
      "   1.52071789e-01 -1.09656528e-01 -1.74397677e-01  2.75099613e-02\n",
      "   1.32211864e-01  1.05161436e-01  5.83325066e-02 -1.45623222e-01\n",
      "  -7.34209716e-02 -2.18120426e-01 -2.17552722e-01  6.45664409e-02\n",
      "   1.93601698e-01 -6.28496930e-02 -3.63652445e-02  3.75326537e-02\n",
      "   1.24334451e-02  2.76361797e-02  1.45841196e-01  1.33165479e-01\n",
      "  -2.50645876e-01  3.82325172e-01  6.41236082e-02  1.14774622e-01\n",
      "  -2.53889561e-01  1.11364879e-01  3.12033687e-02  1.98950365e-01\n",
      "   2.77246684e-01  7.49001056e-02  1.17657587e-01 -1.70508921e-02\n",
      "   1.72194332e-01 -5.59286773e-02 -3.54289301e-02 -1.61671892e-01\n",
      "  -1.51010260e-01  2.04047598e-02 -5.08121923e-02  2.79682547e-01\n",
      "   2.21297946e-02 -1.47126883e-01  6.94229975e-02  1.45566210e-01\n",
      "   2.23076671e-01  4.43539880e-02  1.76292151e-01  1.00988559e-01\n",
      "   3.59836556e-02  1.44206090e-02  2.92409927e-01  2.54927445e-02\n",
      "   1.79355696e-01 -9.63614583e-02 -1.00049160e-01  7.10116252e-02]\n",
      " [ 4.14183997e-02  4.26541232e-02  7.20156506e-02 -3.40431780e-02\n",
      "   4.00128439e-02 -2.50971198e-01  4.96228822e-02  3.84989411e-01\n",
      "  -2.17241168e-01 -1.97845250e-01 -3.44789214e-02 -1.73605934e-01\n",
      "  -5.22806160e-02  5.06767407e-02  1.46647483e-01 -5.14354594e-02\n",
      "   2.84527928e-01 -2.32983783e-01 -1.21871255e-01 -5.31849742e-01\n",
      "   9.99138504e-02 -1.16378568e-01  2.92819291e-01 -2.33517036e-01\n",
      "   1.21196182e-02  1.74911916e-02 -2.47002870e-01  1.29377067e-01\n",
      "  -1.88402474e-01  4.00548950e-02  1.55677468e-01  5.14604747e-02\n",
      "   1.56125933e-01 -3.42911005e-01 -8.65977556e-02  3.07258647e-02\n",
      "   2.52133850e-02 -7.69712850e-02 -2.75655948e-02 -1.29138425e-01\n",
      "   1.56263933e-01 -1.15091011e-01 -3.65744717e-02 -4.37077950e-04\n",
      "   2.72378355e-01 -1.68547705e-01 -2.78306723e-01 -6.13330118e-02\n",
      "   1.39257759e-01  1.96536347e-01  6.72032882e-04  8.80690739e-02\n",
      "   6.80655763e-02 -2.13880613e-01 -5.34110703e-02 -7.85405338e-02\n",
      "   2.09962502e-01 -3.60199697e-02 -7.78260529e-02  1.71859026e-01\n",
      "  -4.14502285e-02  1.22874845e-02  1.17440030e-01  7.47161284e-02\n",
      "  -1.25799894e-01  2.65314013e-01 -3.07455286e-02  1.93614408e-01\n",
      "  -2.17388079e-01  4.43093330e-02  1.43058479e-01  1.16465889e-01\n",
      "   2.04145253e-01  2.02172905e-01  1.62408933e-01 -1.14314757e-01\n",
      "   2.01959655e-01  3.51611562e-02  1.51245847e-01 -3.45005766e-02\n",
      "  -6.15413440e-03  4.32962328e-02 -2.96731051e-02  3.98114085e-01\n",
      "  -6.18037432e-02 -6.82267472e-02  2.83826649e-01  1.20539285e-01\n",
      "   2.28640199e-01  8.24981034e-02  1.52345076e-01  1.12381771e-01\n",
      "  -4.53074239e-02 -7.79738929e-03  3.09363961e-01  1.63473919e-01\n",
      "  -2.64494424e-03 -2.35753089e-01  2.20487230e-02 -4.00580168e-02]\n",
      " [-6.02308065e-02  8.78035128e-02 -9.10979789e-03 -1.53802484e-01\n",
      "   1.20817512e-01 -1.16991676e-01  1.20494887e-03  4.19919372e-01\n",
      "  -3.69984359e-01 -2.69961029e-01  5.73492683e-02 -4.47075099e-01\n",
      "   1.42061889e-01  1.84794173e-01  1.92753389e-01  2.92847846e-02\n",
      "   6.32648110e-01 -8.72526392e-02 -2.53841013e-01 -5.75367808e-01\n",
      "   1.50568292e-01 -1.40198156e-01  3.16240311e-01 -1.15880869e-01\n",
      "   4.29920340e-03  9.89181399e-02 -1.84759378e-01  3.24681729e-01\n",
      "  -2.27547958e-01  1.69316813e-01  1.64781660e-01 -9.78076737e-03\n",
      "   1.28402650e-01 -3.47300529e-01 -2.29364768e-01  7.51487305e-03\n",
      "   2.02176765e-01  1.08827921e-02 -1.05389118e-01 -1.71857830e-02\n",
      "  -1.73735738e-04 -1.79987103e-02 -4.25315499e-02 -2.85018864e-03\n",
      "   2.69939095e-01 -2.11853832e-02  1.49037289e-02 -2.08310142e-01\n",
      "  -2.89111827e-02  3.60151440e-01  7.08137229e-02 -9.16583836e-02\n",
      "   1.15648955e-01 -2.01357290e-01 -1.14002623e-01 -1.95958808e-01\n",
      "   2.52397805e-01 -3.56407799e-02 -8.26818720e-02  2.23168716e-01\n",
      "  -4.76959199e-02 -3.00279353e-02  1.42148092e-01 -1.35147292e-03\n",
      "  -2.34069511e-01  4.86179978e-01 -6.89642802e-02  2.03773916e-01\n",
      "  -8.92021433e-02  7.01843128e-02 -1.24311531e-02  6.97764680e-02\n",
      "   1.30137891e-01  9.53757390e-02  1.78758264e-01 -1.74359262e-01\n",
      "   1.27919957e-01  1.89180329e-01  2.36053824e-01 -7.54938200e-02\n",
      "  -2.23903596e-01  8.80698636e-02 -3.96136604e-02  2.31465384e-01\n",
      "  -1.04900725e-01 -4.80205528e-02  4.39837456e-01 -2.19960660e-02\n",
      "   2.62852520e-01 -2.42743585e-02  3.36703688e-01 -3.61499228e-02\n",
      "  -6.86452463e-02 -1.45029977e-01  1.90472707e-01  8.50679204e-02\n",
      "   2.14112997e-01 -3.35493237e-01 -7.41394907e-02 -9.91151556e-02]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "tokenized_tweets = [tokenize_tweet(tweet) for tweet in df['tweet']]\n",
    "word2vec_model = Word2Vec( sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4, sg=1, hs=0, negative=5, epochs=30)\n",
    "print(word2vec_model)\n",
    "def extract_w2v_features(tweet):\n",
    "    tweet_vectors = [word2vec_model.wv[word] for word in tweet if word in word2vec_model.wv]\n",
    "    if not tweet_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    tweet_vector = np.mean(tweet_vectors, axis=0)\n",
    "    return tweet_vector\n",
    "\n",
    "w2v_features = [extract_w2v_features(tweet) for tweet in tokenized_tweets]\n",
    "w2v_features = np.array(w2v_features)\n",
    "print(w2v_features.shape)\n",
    "print(w2v_features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) Tokenization using BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True , force_download=True)\n",
    "max_len = 128\n",
    "def tokenize_data(texts):\n",
    "    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True,max_length = max_len, return_tensors='pt')\n",
    "    return inputs\n",
    "\n",
    "X_encoded = tokenize_data(df['tweet'].tolist())\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Building and Training BILSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 335ms/step - accuracy: 0.4903 - loss: 0.6941 - val_accuracy: 0.5476 - val_loss: 0.6897\n",
      "Epoch 2/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 146ms/step - accuracy: 0.5675 - loss: 0.6895 - val_accuracy: 0.7143 - val_loss: 0.6880\n",
      "Epoch 3/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.7205 - loss: 0.6788 - val_accuracy: 0.6429 - val_loss: 0.6469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b859669cd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "X = w2v_features\n",
    "y = df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Bidirectional(LSTM(units=32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527ms/step\n",
      "Test accuracy: 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Building and Training BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\salman\\AppData\\Local\\Temp\\ipykernel_1004\\3600398965.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[0]\n",
      "C:\\Users\\salman\\AppData\\Local\\Temp\\ipykernel_1004\\3600398965.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "21/21 [==============================] - 704s 25s/step - loss: 0.4846 - accuracy: 0.8720 - val_loss: 0.3522 - val_accuracy: 0.9048\n",
      "Epoch 2/3\n",
      "21/21 [==============================] - 614s 30s/step - loss: 0.2065 - accuracy: 0.9881 - val_loss: 0.1961 - val_accuracy: 0.9762\n",
      "Epoch 3/3\n",
      "21/21 [==============================] - 931s 44s/step - loss: 0.1288 - accuracy: 0.9940 - val_loss: 0.1710 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x225e80ba000>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_eval = train_test_split(df, test_size=0.2, stratify=df['Label'], random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_data(df, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row[0]\n",
    "        label = row[1]\n",
    "\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = encode_data(df_train, tokenizer)\n",
    "eval_input_ids, eval_attention_masks, eval_labels = encode_data(df_eval, tokenizer)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': train_input_ids,\n",
    "        'attention_mask': train_attention_masks\n",
    "    },\n",
    "    train_labels\n",
    ")).batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': eval_input_ids,\n",
    "        'attention_mask': eval_attention_masks\n",
    "    },\n",
    "    eval_labels\n",
    ")).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "num_train_steps = len(train_dataset) * 3  \n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=0\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(train_dataset, validation_data=eval_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: bert\n",
      "Layer 2: dropout_37\n",
      "Layer 3: classifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i + 1}: {layer.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\salman\\AppData\\Local\\Temp\\ipykernel_1004\\1442344858.py:15: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = row[0]\n",
      "C:\\Users\\salman\\AppData\\Local\\Temp\\ipykernel_1004\\1442344858.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[1]\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: bert, Trainable: False\n",
      "Layer: dropout_113, Trainable: False\n",
      "Layer: classifier, Trainable: False\n",
      "Epoch 1/3\n",
      "21/21 [==============================] - 208s 9s/step - loss: 0.6957 - accuracy: 0.5208 - val_loss: 0.6894 - val_accuracy: 0.5595\n",
      "Epoch 2/3\n",
      "21/21 [==============================] - 178s 9s/step - loss: 0.6979 - accuracy: 0.5298 - val_loss: 0.6894 - val_accuracy: 0.5595\n",
      "Epoch 3/3\n",
      "21/21 [==============================] - 183s 9s/step - loss: 0.6999 - accuracy: 0.5179 - val_loss: 0.6894 - val_accuracy: 0.5595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x226e25ff080>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_eval = train_test_split(df, test_size=0.2, stratify=df['Label'], random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_data(df, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = row[0]\n",
    "        label = row[1]\n",
    "\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoding['input_ids'][0])\n",
    "        attention_masks.append(encoding['attention_mask'][0])\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "train_input_ids, train_attention_masks, train_labels = encode_data(df_train, tokenizer)\n",
    "eval_input_ids, eval_attention_masks, eval_labels = encode_data(df_eval, tokenizer)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': train_input_ids,\n",
    "    'attention_mask': train_attention_masks\n",
    "}, train_labels)).batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': eval_input_ids,\n",
    "    'attention_mask': eval_attention_masks\n",
    "}, eval_labels)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.bert.encoder.layer[-2:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(f\"Layer: {layer.name}, Trainable: {layer.trainable}\")\n",
    "\n",
    "num_train_steps = len(train_dataset) * 3  \n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr=2e-5,                  \n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=0             \n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_dataset, validation_data=eval_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Saving and Compressing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_directory\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_directory\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model_directory', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('bert_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\salman\\\\Desktop\\\\Final Year Project\\\\saved_model.zip'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('saved_model', 'zip', 'saved_model_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5) Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "def predict(sentence):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    logits = model(input_ids, attention_mask=attention_mask)[0]\n",
    "    prediction = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "\n",
    "    return \"Positive\" if prediction == 0 else \"Negative\"\n",
    "\n",
    "print(predict(\"ah i am sorry then who is the.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salman\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Avg Train Loss: 0.5137\n",
      "Avg Eval Loss: 0.3302 | Accuracy: 0.8571\n",
      "Epoch 2/3 | Avg Train Loss: 0.1475\n",
      "Avg Eval Loss: 0.2399 | Accuracy: 0.9167\n",
      "Epoch 3/3 | Avg Train Loss: 0.0467\n",
      "Avg Eval Loss: 0.3506 | Accuracy: 0.8810\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class HateSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 0]\n",
    "        label = self.df.iloc[idx, 1]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "df_train, df_eval = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Label'])\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "train_dataset = HateSpeechDataset(df_train, tokenizer)\n",
    "eval_dataset = HateSpeechDataset(df_eval, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(eval_loader)\n",
    "    accuracy = total_correct / len(df_eval)\n",
    "    print(f\"Avg Eval Loss: {avg_eval_loss:.4f} | Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
